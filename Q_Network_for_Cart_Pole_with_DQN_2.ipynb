{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 10:12:09,525] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input and output size based on the Env\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size= env.action_space.n\n",
    "\n",
    "dis = 0.9\n",
    "REPLAY_MEMORY = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\"):\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        self._build_network()\n",
    "    \n",
    "    def _build_network(self, h_size=10, l_rate=1e-1):\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            \n",
    "            # First layer of weights\n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1 = tf.nn.tanh(tf.matmul(self._X,W1))\n",
    "            \n",
    "            # Second layer of weights\n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, self.output_size],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #Q prediction\n",
    "            self._Qpred = tf.matmul(layer1, W2)\n",
    "            \n",
    "        # We need to define the parts of the network needed for learning a policy\n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        # Loss Function\n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        \n",
    "        # Learning\n",
    "        self._train = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1, self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X:x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X:x_stack, self._Y: y_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replay_train(mainDQN, targetDQN, train_batch):\n",
    "    x_stack = np.empty(0).reshape(0, mainDQN.input_size)\n",
    "    y_stack = np.empty(0).reshape(0, mainDQN.output_size)\n",
    "    \n",
    "    # Get stored information from the buffer\n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)\n",
    "        # terminal?\n",
    "        if done:\n",
    "            # Update Q, and no Qs+1, sice it's a terminal state\n",
    "            Q[0, action] = reward\n",
    "        else:\n",
    "            # Obtain the Q_s1 values by feeding the new state through our network \n",
    "            Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state))\n",
    "        \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "    \n",
    "    # Train out network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(x_stack, y_stack)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "        \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bot_play(mainDQN):\n",
    "    # See our trained network in action\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        # env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s, reward, done, _ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        if reward_sum > 100000 or done:\n",
    "            print(\"Total score:{}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 steps:13\n",
      "Episode:1 steps:13\n",
      "('Loss:', 422.15805, ', before Loss:', 1000)\n",
      "Episode:2 steps:14\n",
      "Episode:3 steps:14\n",
      "Episode:4 steps:23\n",
      "Episode:5 steps:19\n",
      "Episode:6 steps:21\n",
      "Episode:7 steps:14\n",
      "Episode:8 steps:43\n",
      "Episode:9 steps:17\n",
      "Episode:10 steps:22\n",
      "Episode:11 steps:14\n",
      "('Loss:', 1005.8701, ', before Loss:', 422.15805)\n",
      "Episode:12 steps:15\n",
      "Episode:13 steps:11\n",
      "Episode:14 steps:10\n",
      "Episode:15 steps:10\n",
      "Episode:16 steps:18\n",
      "Episode:17 steps:12\n",
      "Episode:18 steps:13\n",
      "Episode:19 steps:10\n",
      "Episode:20 steps:10\n",
      "Episode:21 steps:9\n",
      "('Loss:', 17.578108, ', before Loss:', 422.15805)\n",
      "Episode:22 steps:18\n",
      "Episode:23 steps:26\n",
      "Episode:24 steps:33\n",
      "Episode:25 steps:25\n",
      "Episode:26 steps:18\n",
      "Episode:27 steps:19\n",
      "Episode:28 steps:18\n",
      "Episode:29 steps:28\n",
      "Episode:30 steps:27\n",
      "Episode:31 steps:25\n",
      "('Loss:', 594.35474, ', before Loss:', 17.578108)\n",
      "Episode:32 steps:19\n",
      "Episode:33 steps:18\n",
      "Episode:34 steps:14\n",
      "Episode:35 steps:14\n",
      "Episode:36 steps:30\n",
      "Episode:37 steps:15\n",
      "Episode:38 steps:18\n",
      "Episode:39 steps:26\n",
      "Episode:40 steps:21\n",
      "Episode:41 steps:27\n",
      "('Loss:', 612.53687, ', before Loss:', 17.578108)\n",
      "Episode:42 steps:50\n",
      "Episode:43 steps:43\n",
      "Episode:44 steps:30\n",
      "Episode:45 steps:64\n",
      "Episode:46 steps:48\n",
      "Episode:47 steps:74\n",
      "Episode:48 steps:37\n",
      "Episode:49 steps:47\n",
      "Episode:50 steps:52\n",
      "Episode:51 steps:38\n",
      "('Loss:', 509.8916, ', before Loss:', 17.578108)\n",
      "Episode:52 steps:34\n",
      "Episode:53 steps:34\n",
      "Episode:54 steps:101\n",
      "Episode:55 steps:37\n",
      "Episode:56 steps:95\n",
      "Episode:57 steps:80\n",
      "Episode:58 steps:45\n",
      "Episode:59 steps:32\n",
      "Episode:60 steps:36\n",
      "Episode:61 steps:35\n",
      "('Loss:', 2.2949874, ', before Loss:', 17.578108)\n",
      "Episode:62 steps:31\n",
      "Episode:63 steps:25\n",
      "Episode:64 steps:24\n",
      "Episode:65 steps:41\n",
      "Episode:66 steps:22\n",
      "Episode:67 steps:25\n",
      "Episode:68 steps:30\n",
      "Episode:69 steps:75\n",
      "Episode:70 steps:36\n",
      "Episode:71 steps:51\n",
      "('Loss:', 2.9395092, ', before Loss:', 2.2949874)\n",
      "Episode:72 steps:13\n",
      "Episode:73 steps:8\n",
      "Episode:74 steps:10\n",
      "Episode:75 steps:9\n",
      "Episode:76 steps:9\n",
      "Episode:77 steps:9\n",
      "Episode:78 steps:9\n",
      "Episode:79 steps:12\n",
      "Episode:80 steps:12\n",
      "Episode:81 steps:9\n",
      "('Loss:', 1.5033299, ', before Loss:', 2.2949874)\n",
      "Episode:82 steps:10\n",
      "Episode:83 steps:9\n",
      "Episode:84 steps:8\n",
      "Episode:85 steps:12\n",
      "Episode:86 steps:8\n",
      "Episode:87 steps:22\n",
      "Episode:88 steps:10\n",
      "Episode:89 steps:15\n",
      "Episode:90 steps:11\n",
      "Episode:91 steps:14\n",
      "('Loss:', 460.67685, ', before Loss:', 1.5033299)\n",
      "Episode:92 steps:49\n",
      "Episode:93 steps:50\n",
      "Episode:94 steps:56\n",
      "Episode:95 steps:69\n",
      "Episode:96 steps:63\n",
      "Episode:97 steps:74\n",
      "Episode:98 steps:48\n",
      "Episode:99 steps:50\n",
      "Episode:100 steps:83\n",
      "Episode:101 steps:78\n",
      "('Loss:', 1.1017548, ', before Loss:', 1.5033299)\n",
      "Episode:102 steps:51\n",
      "Episode:103 steps:33\n",
      "Episode:104 steps:47\n",
      "Episode:105 steps:38\n",
      "Episode:106 steps:39\n",
      "Episode:107 steps:25\n",
      "Episode:108 steps:88\n",
      "Episode:109 steps:62\n",
      "Episode:110 steps:42\n",
      "Episode:111 steps:52\n",
      "('Loss:', 2.4225426, ', before Loss:', 1.1017548)\n",
      "Episode:112 steps:39\n",
      "Episode:113 steps:33\n",
      "Episode:114 steps:35\n",
      "Episode:115 steps:31\n",
      "Episode:116 steps:47\n",
      "Episode:117 steps:31\n",
      "Episode:118 steps:35\n",
      "Episode:119 steps:35\n",
      "Episode:120 steps:32\n",
      "Episode:121 steps:83\n",
      "('Loss:', 487.48688, ', before Loss:', 1.1017548)\n",
      "Episode:122 steps:20\n",
      "Episode:123 steps:15\n",
      "Episode:124 steps:14\n",
      "Episode:125 steps:15\n",
      "Episode:126 steps:14\n",
      "Episode:127 steps:22\n",
      "Episode:128 steps:21\n",
      "Episode:129 steps:15\n",
      "Episode:130 steps:25\n",
      "Episode:131 steps:13\n",
      "('Loss:', 2.8434892, ', before Loss:', 1.1017548)\n",
      "Episode:132 steps:44\n",
      "Episode:133 steps:24\n",
      "Episode:134 steps:22\n",
      "Episode:135 steps:23\n",
      "Episode:136 steps:25\n",
      "Episode:137 steps:27\n",
      "Episode:138 steps:48\n",
      "Episode:139 steps:24\n",
      "Episode:140 steps:44\n",
      "Episode:141 steps:24\n",
      "('Loss:', 1.3327097, ', before Loss:', 1.1017548)\n",
      "Episode:142 steps:9\n",
      "Episode:143 steps:10\n",
      "Episode:144 steps:11\n",
      "Episode:145 steps:10\n",
      "Episode:146 steps:10\n",
      "Episode:147 steps:12\n",
      "Episode:148 steps:11\n",
      "Episode:149 steps:12\n",
      "Episode:150 steps:15\n",
      "Episode:151 steps:15\n",
      "('Loss:', 0.2156733, ', before Loss:', 1.1017548)\n",
      "Episode:152 steps:28\n",
      "Episode:153 steps:23\n",
      "Episode:154 steps:23\n",
      "Episode:155 steps:28\n",
      "Episode:156 steps:39\n",
      "Episode:157 steps:30\n",
      "Episode:158 steps:22\n",
      "Episode:159 steps:42\n",
      "Episode:160 steps:24\n",
      "Episode:161 steps:50\n",
      "('Loss:', 3.1624756, ', before Loss:', 0.2156733)\n",
      "Episode:162 steps:14\n",
      "Episode:163 steps:13\n",
      "Episode:164 steps:16\n",
      "Episode:165 steps:15\n",
      "Episode:166 steps:12\n",
      "Episode:167 steps:14\n",
      "Episode:168 steps:20\n",
      "Episode:169 steps:14\n",
      "Episode:170 steps:12\n",
      "Episode:171 steps:17\n",
      "('Loss:', 479.26959, ', before Loss:', 0.2156733)\n",
      "Episode:172 steps:10\n",
      "Episode:173 steps:8\n",
      "Episode:174 steps:9\n",
      "Episode:175 steps:9\n",
      "Episode:176 steps:11\n",
      "Episode:177 steps:9\n",
      "Episode:178 steps:10\n",
      "Episode:179 steps:8\n",
      "Episode:180 steps:10\n",
      "Episode:181 steps:8\n",
      "('Loss:', 3.7059503, ', before Loss:', 0.2156733)\n",
      "Episode:182 steps:9\n",
      "Episode:183 steps:9\n",
      "Episode:184 steps:9\n",
      "Episode:185 steps:9\n",
      "Episode:186 steps:9\n",
      "Episode:187 steps:9\n",
      "Episode:188 steps:9\n",
      "Episode:189 steps:9\n",
      "Episode:190 steps:9\n",
      "Episode:191 steps:9\n",
      "('Loss:', 493.41046, ', before Loss:', 0.2156733)\n",
      "Episode:192 steps:60\n",
      "Episode:193 steps:55\n",
      "Episode:194 steps:51\n",
      "Episode:195 steps:45\n",
      "Episode:196 steps:50\n",
      "Episode:197 steps:70\n",
      "Episode:198 steps:46\n",
      "Episode:199 steps:66\n",
      "Episode:200 steps:80\n",
      "Episode:201 steps:43\n",
      "('Loss:', 3.4184055, ', before Loss:', 0.2156733)\n",
      "Episode:202 steps:28\n",
      "Episode:203 steps:20\n",
      "Episode:204 steps:48\n",
      "Episode:205 steps:20\n",
      "Episode:206 steps:22\n",
      "Episode:207 steps:29\n",
      "Episode:208 steps:20\n",
      "Episode:209 steps:21\n",
      "Episode:210 steps:30\n",
      "Episode:211 steps:39\n",
      "('Loss:', 480.14316, ', before Loss:', 0.2156733)\n",
      "Episode:212 steps:13\n",
      "Episode:213 steps:17\n",
      "Episode:214 steps:21\n",
      "Episode:215 steps:19\n",
      "Episode:216 steps:15\n",
      "Episode:217 steps:21\n",
      "Episode:218 steps:17\n",
      "Episode:219 steps:23\n",
      "Episode:220 steps:19\n",
      "Episode:221 steps:23\n",
      "('Loss:', 16.113485, ', before Loss:', 0.2156733)\n",
      "Episode:222 steps:19\n",
      "Episode:223 steps:30\n",
      "Episode:224 steps:24\n",
      "Episode:225 steps:21\n",
      "Episode:226 steps:69\n",
      "Episode:227 steps:30\n",
      "Episode:228 steps:55\n",
      "Episode:229 steps:57\n",
      "Episode:230 steps:21\n",
      "Episode:231 steps:21\n",
      "('Loss:', 520.06238, ', before Loss:', 0.2156733)\n",
      "Episode:232 steps:11\n",
      "Episode:233 steps:9\n",
      "Episode:234 steps:10\n",
      "Episode:235 steps:10\n",
      "Episode:236 steps:10\n",
      "Episode:237 steps:10\n",
      "Episode:238 steps:9\n",
      "Episode:239 steps:11\n",
      "Episode:240 steps:9\n",
      "Episode:241 steps:11\n",
      "('Loss:', 5.6806607, ', before Loss:', 0.2156733)\n",
      "Episode:242 steps:34\n",
      "Episode:243 steps:48\n",
      "Episode:244 steps:79\n",
      "Episode:245 steps:64\n",
      "Episode:246 steps:42\n",
      "Episode:247 steps:41\n",
      "Episode:248 steps:98\n",
      "Episode:249 steps:40\n",
      "Episode:250 steps:61\n",
      "Episode:251 steps:53\n",
      "('Loss:', 3.373431, ', before Loss:', 0.2156733)\n",
      "Episode:252 steps:17\n",
      "Episode:253 steps:24\n",
      "Episode:254 steps:17\n",
      "Episode:255 steps:27\n",
      "Episode:256 steps:17\n",
      "Episode:257 steps:15\n",
      "Episode:258 steps:20\n",
      "Episode:259 steps:18\n",
      "Episode:260 steps:20\n",
      "Episode:261 steps:11\n",
      "('Loss:', 521.56641, ', before Loss:', 0.2156733)\n",
      "Episode:262 steps:59\n",
      "Episode:263 steps:22\n",
      "Episode:264 steps:25\n",
      "Episode:265 steps:36\n",
      "Episode:266 steps:31\n",
      "Episode:267 steps:24\n",
      "Episode:268 steps:26\n",
      "Episode:269 steps:37\n",
      "Episode:270 steps:31\n",
      "Episode:271 steps:23\n",
      "('Loss:', 1035.8915, ', before Loss:', 0.2156733)\n",
      "Episode:272 steps:27\n",
      "Episode:273 steps:48\n",
      "Episode:274 steps:59\n",
      "Episode:275 steps:59\n",
      "Episode:276 steps:81\n",
      "Episode:277 steps:30\n",
      "Episode:278 steps:21\n",
      "Episode:279 steps:35\n",
      "Episode:280 steps:26\n",
      "Episode:281 steps:48\n",
      "('Loss:', 0.96663463, ', before Loss:', 0.2156733)\n",
      "Episode:282 steps:25\n",
      "Episode:283 steps:35\n",
      "Episode:284 steps:18\n",
      "Episode:285 steps:26\n",
      "Episode:286 steps:23\n",
      "Episode:287 steps:24\n",
      "Episode:288 steps:21\n",
      "Episode:289 steps:29\n",
      "Episode:290 steps:36\n",
      "Episode:291 steps:36\n",
      "('Loss:', 4.5780082, ', before Loss:', 0.2156733)\n",
      "Episode:292 steps:122\n",
      "Episode:293 steps:1216\n",
      "Episode:294 steps:861\n",
      "Episode:295 steps:650\n",
      "Episode:296 steps:168\n",
      "Episode:297 steps:106\n",
      "Episode:298 steps:264\n",
      "Episode:299 steps:238\n",
      "Episode:300 steps:137\n",
      "Episode:301 steps:3570\n",
      "('Loss:', 13.443576, ', before Loss:', 0.2156733)\n",
      "Episode:302 steps:10001\n",
      "Episode:303 steps:10001\n",
      "Episode:304 steps:10001\n",
      "Episode:305 steps:10001\n",
      "Episode:306 steps:10001\n",
      "Episode:307 steps:10001\n",
      "Total score:100001.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    max_episodes = 5000\n",
    "    before_loss = 1000\n",
    "    stop_count = 0\n",
    "    \n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque()\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "    targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # initial copy q_net - > target_net\n",
    "    copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
    "    sess.run(copy_ops)\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        e = 1. / ((episode / 10) + 1)\n",
    "        done = False\n",
    "        step_count = 0    \n",
    "        \n",
    "        state = env.reset()\n",
    "            \n",
    "        while not done:\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Choose an action by greedily from the Q-network\n",
    "                action = np.argmax(mainDQN.predict(state))\n",
    "                \n",
    "            # Get new state and reward from environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                reward = -100\n",
    "           \n",
    "            # Save the experienve to our buffer\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                replay_buffer.popleft()\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            if step_count > 10000:\n",
    "                stop_count += 1\n",
    "                break\n",
    "            else:\n",
    "                stop_cout = 0\n",
    "                \n",
    "                    \n",
    "        print(\"Episode:{} steps:{}\".format(episode, step_count))\n",
    "        if step_count > 10000:\n",
    "            if stop_count > 5:\n",
    "                break\n",
    "            pass\n",
    "            \n",
    "        # train every 10 episodes\n",
    "        if episode % 10 == 1:\n",
    "            for _ in range(50):\n",
    "                # Minibatch works better\n",
    "                minibatch = random.sample(replay_buffer, 10)\n",
    "                loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "            print(\"Loss:\",loss,\", before Loss:\",before_loss)\n",
    "            if loss < before_loss:\n",
    "                before_loss = loss\n",
    "                sess.run(copy_ops)\n",
    "\n",
    "    bot_play(mainDQN)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
