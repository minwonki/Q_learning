{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-24 15:55:55,757] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "# Input and output size based on the Env\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "learning_rate = 0.1\n",
    "print input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theses lines establish the feed-forward part of the network used to choose actions\n",
    "# state input\n",
    "X = tf.placeholder(tf.float32, [None,input_size], name=\"input_x\") # state input\n",
    "# weight\n",
    "W = tf.get_variable(\"W\", shape=[input_size, output_size], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "Qpred = tf.matmul(X,W) # out Q prediction\n",
    "Y = tf.placeholder(shape=[None,output_size], dtype=tf.float32) # Y label\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Q-learning related parameters\n",
    "dis = 0.99\n",
    "num_episodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lists to contrain total rewards and steps per episode\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 steps:10\n",
      "Episode:1 steps:9\n",
      "Episode:2 steps:65\n",
      "Episode:3 steps:42\n",
      "Episode:4 steps:24\n",
      "Episode:5 steps:24\n",
      "Episode:6 steps:38\n",
      "Episode:7 steps:15\n",
      "Episode:8 steps:10\n",
      "Episode:9 steps:10\n",
      "Episode:10 steps:9\n",
      "Episode:11 steps:9\n",
      "Episode:12 steps:12\n",
      "Episode:13 steps:34\n",
      "Episode:14 steps:18\n",
      "Episode:15 steps:26\n",
      "Episode:16 steps:38\n",
      "Episode:17 steps:23\n",
      "Episode:18 steps:27\n",
      "Episode:19 steps:24\n",
      "Episode:20 steps:29\n",
      "Episode:21 steps:31\n",
      "Episode:22 steps:62\n",
      "Episode:23 steps:24\n",
      "Episode:24 steps:32\n",
      "Episode:25 steps:10\n",
      "Episode:26 steps:12\n",
      "Episode:27 steps:16\n",
      "Episode:28 steps:35\n",
      "Episode:29 steps:25\n",
      "Episode:30 steps:21\n",
      "Episode:31 steps:18\n",
      "Episode:32 steps:15\n",
      "Episode:33 steps:43\n",
      "Episode:34 steps:15\n",
      "Episode:35 steps:31\n",
      "Episode:36 steps:25\n",
      "Episode:37 steps:51\n",
      "Episode:38 steps:45\n",
      "Episode:39 steps:29\n",
      "Episode:40 steps:22\n",
      "Episode:41 steps:55\n",
      "Episode:42 steps:19\n",
      "Episode:43 steps:11\n",
      "Episode:44 steps:10\n",
      "Episode:45 steps:9\n",
      "Episode:46 steps:10\n",
      "Episode:47 steps:47\n",
      "Episode:48 steps:23\n",
      "Episode:49 steps:67\n",
      "Episode:50 steps:22\n",
      "Episode:51 steps:34\n",
      "Episode:52 steps:23\n",
      "Episode:53 steps:29\n",
      "Episode:54 steps:50\n",
      "Episode:55 steps:33\n",
      "Episode:56 steps:69\n",
      "Episode:57 steps:30\n",
      "Episode:58 steps:30\n",
      "Episode:59 steps:17\n",
      "Episode:60 steps:30\n",
      "Episode:61 steps:25\n",
      "Episode:62 steps:30\n",
      "Episode:63 steps:16\n",
      "Episode:64 steps:24\n",
      "Episode:65 steps:34\n",
      "Episode:66 steps:27\n",
      "Episode:67 steps:26\n",
      "Episode:68 steps:21\n",
      "Episode:69 steps:25\n",
      "Episode:70 steps:36\n",
      "Episode:71 steps:40\n",
      "Episode:72 steps:46\n",
      "Episode:73 steps:44\n",
      "Episode:74 steps:82\n",
      "Episode:75 steps:127\n",
      "Episode:76 steps:42\n",
      "Episode:77 steps:124\n",
      "Episode:78 steps:42\n",
      "Episode:79 steps:51\n",
      "Episode:80 steps:77\n",
      "Episode:81 steps:36\n",
      "Episode:82 steps:39\n",
      "Episode:83 steps:26\n",
      "Episode:84 steps:73\n",
      "Episode:85 steps:29\n",
      "Episode:86 steps:29\n",
      "Episode:87 steps:27\n",
      "Episode:88 steps:35\n",
      "Episode:89 steps:29\n",
      "Episode:90 steps:38\n",
      "Episode:91 steps:21\n",
      "Episode:92 steps:14\n",
      "Episode:93 steps:9\n",
      "Episode:94 steps:9\n",
      "Episode:95 steps:25\n",
      "Episode:96 steps:41\n",
      "Episode:97 steps:20\n",
      "Episode:98 steps:23\n",
      "Episode:99 steps:21\n",
      "Episode:100 steps:26\n",
      "Episode:101 steps:44\n",
      "Episode:102 steps:54\n",
      "Episode:103 steps:33\n",
      "Episode:104 steps:14\n",
      "Episode:105 steps:22\n",
      "Episode:106 steps:38\n",
      "Episode:107 steps:24\n",
      "Episode:108 steps:30\n",
      "Episode:109 steps:32\n",
      "Episode:110 steps:29\n",
      "Episode:111 steps:12\n",
      "Episode:112 steps:21\n",
      "Episode:113 steps:20\n",
      "Episode:114 steps:42\n",
      "Episode:115 steps:29\n",
      "Episode:116 steps:22\n",
      "Episode:117 steps:27\n",
      "Episode:118 steps:30\n",
      "Episode:119 steps:50\n",
      "Episode:120 steps:32\n",
      "Episode:121 steps:29\n",
      "Episode:122 steps:32\n",
      "Episode:123 steps:22\n",
      "Episode:124 steps:17\n",
      "Episode:125 steps:47\n",
      "Episode:126 steps:50\n",
      "Episode:127 steps:10\n",
      "Episode:128 steps:22\n",
      "Episode:129 steps:20\n",
      "Episode:130 steps:17\n",
      "Episode:131 steps:41\n",
      "Episode:132 steps:34\n",
      "Episode:133 steps:30\n",
      "Episode:134 steps:48\n",
      "Episode:135 steps:23\n",
      "Episode:136 steps:39\n",
      "Episode:137 steps:13\n",
      "Episode:138 steps:57\n",
      "Episode:139 steps:23\n",
      "Episode:140 steps:16\n",
      "Episode:141 steps:33\n",
      "Episode:142 steps:33\n",
      "Episode:143 steps:21\n",
      "Episode:144 steps:33\n",
      "Episode:145 steps:18\n",
      "Episode:146 steps:12\n",
      "Episode:147 steps:23\n",
      "Episode:148 steps:39\n",
      "Episode:149 steps:25\n",
      "Episode:150 steps:33\n",
      "Episode:151 steps:39\n",
      "Episode:152 steps:22\n",
      "Episode:153 steps:41\n",
      "Episode:154 steps:34\n",
      "Episode:155 steps:37\n",
      "Episode:156 steps:39\n",
      "Episode:157 steps:43\n",
      "Episode:158 steps:20\n",
      "Episode:159 steps:40\n",
      "Episode:160 steps:17\n",
      "Episode:161 steps:19\n",
      "Episode:162 steps:35\n",
      "Episode:163 steps:31\n",
      "Episode:164 steps:15\n",
      "Episode:165 steps:37\n",
      "Episode:166 steps:25\n",
      "Episode:167 steps:26\n",
      "Episode:168 steps:21\n",
      "Episode:169 steps:26\n",
      "Episode:170 steps:24\n",
      "Episode:171 steps:22\n",
      "Episode:172 steps:31\n",
      "Episode:173 steps:26\n",
      "Episode:174 steps:30\n",
      "Episode:175 steps:28\n",
      "Episode:176 steps:29\n",
      "Episode:177 steps:72\n",
      "Episode:178 steps:37\n",
      "Episode:179 steps:53\n",
      "Episode:180 steps:34\n",
      "Episode:181 steps:59\n",
      "Episode:182 steps:40\n",
      "Episode:183 steps:22\n",
      "Episode:184 steps:27\n",
      "Episode:185 steps:36\n",
      "Episode:186 steps:25\n",
      "Episode:187 steps:27\n",
      "Episode:188 steps:33\n",
      "Episode:189 steps:52\n",
      "Episode:190 steps:28\n",
      "Episode:191 steps:44\n",
      "Episode:192 steps:51\n",
      "Episode:193 steps:26\n",
      "Episode:194 steps:35\n",
      "Episode:195 steps:13\n",
      "Episode:196 steps:16\n",
      "Episode:197 steps:20\n",
      "Episode:198 steps:22\n",
      "Episode:199 steps:37\n"
     ]
    }
   ],
   "source": [
    "# Q-network training\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    e = 1. / ((i / 50) + 10)\n",
    "    step_count = 0\n",
    "    done = False\n",
    "    \n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.reshape(s, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "        Qs = sess.run(Qpred, feed_dict={X:x})\n",
    "        if np.random.rand(1) < e:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(Qs)\n",
    "            \n",
    "        # Get new state and reward from environment\n",
    "        s1, reward, done, _ = env.step(a)\n",
    "        if done:\n",
    "            # Update Q, and no Qs+1, sice it's a terminal state\n",
    "            Qs[0, a] = -100\n",
    "        else:\n",
    "            x1 = np.reshape(s1, [1, input_size])\n",
    "            # Obtain the Q_s1 values by feeding the new state through our network \n",
    "            Qs1 = sess.run(Qpred, feed_dict={X: x1})\n",
    "            # update Q\n",
    "            Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "        \n",
    "        # Train our network using using target and predicted Q values on each episode    \n",
    "        sess.run(train, feed_dict={X: x, Y:Qs})\n",
    "        s = s1\n",
    "    rList.append(step_count)\n",
    "    print(\"Episode:{} steps:{}\".format(i, step_count))\n",
    "    if len(rList) > 10 and np.mean(rList[-10:]) > 200:\n",
    "        break\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score:28.0\n"
     ]
    }
   ],
   "source": [
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    #env.render()\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Qs = sess.run(Qpred, feed_dict={X:x})\n",
    "    a = np.argmax(Qs)\n",
    "    \n",
    "    observation, reward, done, _ = env.step(a)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score:{}\".format(reward_sum))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
